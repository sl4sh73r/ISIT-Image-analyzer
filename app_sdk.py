from flask import Flask, request, jsonify, render_template
import lmstudio as lms
import base64
import os
import time
from werkzeug.utils import secure_filename

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB

os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# –ú–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
MODELS = ["qwen/qwen3-vl-4b", "google/gemma-3-4b"]
ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'bmp', 'webp'}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç LM Studio
lms_client = None
current_loaded_model = None

def init_lms_client():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ LM Studio"""
    global lms_client
    try:
        lms_client = lms.Client()
        print("‚úì LM Studio –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        return True
    except Exception as e:
        print(f"‚úó –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LM Studio: {e}")
        return False

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def get_current_model():
    """–ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â—É—é –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
    global lms_client, current_loaded_model
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º list() –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        models = lms_client.llm.list()
        if models:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å
            first_model = models[0]
            model_id = first_model.identifier if hasattr(first_model, 'identifier') else str(first_model)
            current_loaded_model = model_id
            return model_id
        return None
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏: {e}")
        return current_loaded_model

def analyze_image_with_model(image_path, model_name):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é —É–∫–∞–∑–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ LM Studio SDK"""
    global lms_client
    
    try:
        print(f"\n{'='*60}")
        print(f"–ó–∞–≥—Ä—É–∂–∞—é –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å –ø–æ–º–æ—â—å—é: {model_name}")
        print(f"{'='*60}")
        
        # –ß–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        with open(image_path, "rb") as img_file:
            img_b64 = base64.b64encode(img_file.read()).decode("utf-8")
        
        ext = image_path.rsplit('.', 1)[1].lower()
        mime_type = f"image/{ext if ext != 'jpg' else 'jpeg'}"
        
        # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞
        start_time = time.time()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ SDK —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        print(f"‚è≥ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å {model_name}...")
        model = lms_client.llm.load_new_instance(
            model_name,
            config={
                "contextLength": 8192,
                "gpu": {
                    "ratio": 1.0  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—é –¥–æ—Å—Ç—É–ø–Ω—É—é GPU –ø–∞–º—è—Ç—å
                }
            }
        )
        print(f"‚úì –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:{mime_type};base64,{img_b64}"
                        }
                    },
                    {
                        "type": "text",
                        "text": "–û–ø—Ä–µ–¥–µ–ª–∏, —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–æ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ. –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º –∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–æ–π —Ñ—Ä–∞–∑–æ–π ‚Äî —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."
                    }
                ]
            }
        ]
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        print(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...")
        response = model.respond(
            messages,
            config={
                "temperature": 0.2,
                "maxTokens": 30
            }
        )
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º—è
        end_time = time.time()
        processing_time = round(end_time - start_time, 3)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç
        entity = response.content if hasattr(response, 'content') else str(response)
        entity = entity.strip()
        
        print(f"‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç: {entity}")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è: {processing_time}—Å")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        metrics = {
            "entity": entity,
            "model": model_name,
            "processing_time": processing_time,
            "temperature": 0.2,
            "max_tokens": 30
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ–∫–µ–Ω–æ–≤, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        if hasattr(response, 'stats'):
            stats = response.stats
            if hasattr(stats, 'prompt_tokens'):
                metrics["prompt_tokens"] = stats.prompt_tokens
            if hasattr(stats, 'completion_tokens'):
                metrics["completion_tokens"] = stats.completion_tokens
            if hasattr(stats, 'total_tokens'):
                metrics["total_tokens"] = stats.total_tokens
                
            # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å
            if "completion_tokens" in metrics and processing_time > 0:
                metrics["tokens_per_second"] = round(metrics["completion_tokens"] / processing_time, 2)
        
        # –í—ã–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        print(f"üîÑ –í—ã–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å {model_name}...")
        try:
            model.unload()
            print(f"‚úì –ú–æ–¥–µ–ª—å –≤—ã–≥—Ä—É–∂–µ–Ω–∞")
        except:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å")
        
        return metrics
        
    except Exception as e:
        error_msg = str(e)
        print(f"‚úó –û—à–∏–±–∫–∞: {error_msg}")
        
        return {
            "error": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –º–æ–¥–µ–ª—å—é {model_name}: {error_msg}",
            "model": model_name
        }

def calculate_comparison(results):
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏"""
    comparison = {}
    
    successful_results = [r for r in results if "error" not in r]
    
    if len(successful_results) < 2:
        return comparison
    
    model1, model2 = successful_results[0], successful_results[1]
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
    if "processing_time" in model1 and "processing_time" in model2:
        time_diff = abs(model1["processing_time"] - model2["processing_time"])
        faster_model = model1["model"] if model1["processing_time"] < model2["processing_time"] else model2["model"]
        comparison["time_difference"] = round(time_diff, 3)
        comparison["faster_model"] = faster_model
        
        avg_time = (model1["processing_time"] + model2["processing_time"]) / 2
        if avg_time > 0:
            comparison["time_difference_percent"] = round((time_diff / avg_time) * 100, 1)
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
    if "tokens_per_second" in model1 and "tokens_per_second" in model2:
        faster_tokens_model = model1["model"] if model1["tokens_per_second"] > model2["tokens_per_second"] else model2["model"]
        comparison["faster_tokens_model"] = faster_tokens_model
        comparison["tokens_per_second_diff"] = round(abs(model1["tokens_per_second"] - model2["tokens_per_second"]), 2)
    
    if "total_tokens" in model1 and "total_tokens" in model2:
        comparison["total_tokens_diff"] = abs(model1["total_tokens"] - model2["total_tokens"])
        comparison["more_efficient_model"] = model1["model"] if model1["total_tokens"] < model2["total_tokens"] else model2["model"]
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤
    if "entity" in model1 and "entity" in model2:
        comparison["answers_match"] = model1["entity"].lower() == model2["entity"].lower()
        comparison["answer_similarity"] = "identical" if comparison["answers_match"] else "different"
    
    return comparison

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/api/check-models', methods=['GET'])
def check_models():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    global lms_client
    
    try:
        if not lms_client:
            init_lms_client()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        loaded_models = lms_client.llm.list() if lms_client else []
        loaded_ids = [m.identifier if hasattr(m, 'identifier') else str(m) for m in loaded_models]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –Ω–∞—à–∏—Ö –º–æ–¥–µ–ª–µ–π
        available_models = []
        for model_name in MODELS:
            is_loaded = any(model_name in model_id for model_id in loaded_ids)
            available_models.append({
                'name': model_name,
                'short_name': model_name.split('/')[1] if '/' in model_name else model_name,
                'available': is_loaded,
                'currently_loaded': is_loaded
            })
        
        loaded_count = sum(1 for m in available_models if m['available'])
        current = loaded_ids[0] if loaded_ids else None
        
        return jsonify({
            'status': 'ok',
            'models': available_models,
            'loaded_count': loaded_count,
            'total_count': len(MODELS),
            'all_loaded': loaded_count == len(MODELS),
            'current_model': current,
            'auto_switching': True,
            'sdk_enabled': True,
            'note': 'üîÑ –ú–æ–¥–µ–ª–∏ –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è —á–µ—Ä–µ–∑ LM Studio SDK'
        })
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e),
            'suggestion': '–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ LM Studio –∑–∞–ø—É—â–µ–Ω'
        }), 500

@app.route('/api/active-model', methods=['GET'])
def get_active_model():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –∞–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å"""
    try:
        current = get_current_model()
        
        return jsonify({
            'success': True,
            'active_model': current,
            'active_model_short': current.split('/')[1] if current and '/' in current else current,
            'available_models': MODELS,
            'auto_switching': True,
            'sdk_enabled': True,
            'instructions': {
                'step1': 'SDK –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª–∏',
                'step2': '–ü—Ä–æ—Å—Ç–æ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ',
                'step3': '–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–±–µ –º–æ–¥–µ–ª–∏',
                'step4': '–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å—Ä–∞–≤–Ω–µ–Ω—ã'
            }
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/analyze', methods=['POST'])
def analyze_image():
    """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ–±–µ–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏"""
    if 'image' not in request.files:
        return jsonify({'error': '–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}), 400
    
    file = request.files['image']
    
    if file.filename == '':
        return jsonify({'error': '–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω'}), 400
    
    if not allowed_file(file.filename):
        return jsonify({'error': '–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞'}), 400
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if not lms_client:
            if not init_lms_client():
                return jsonify({
                    'success': False,
                    'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ LM Studio'
                }), 500
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(filepath)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±–µ–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
        results = []
        for model_name in MODELS:
            result = analyze_image_with_model(filepath, model_name)
            results.append(result)
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏
            time.sleep(1)
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        os.remove(filepath)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å
        successful_results = [r for r in results if "error" not in r]
        
        if not successful_results:
            return jsonify({
                'success': False,
                'error': '–û–±–µ –º–æ–¥–µ–ª–∏ –≤–µ—Ä–Ω—É–ª–∏ –æ—à–∏–±–∫—É',
                'results': results
            }), 500
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
        comparison = calculate_comparison(results)
        
        return jsonify({
            'success': True,
            'results': results,
            'comparison': comparison,
            'models_analyzed': len(successful_results),
            'models_failed': len(results) - len(successful_results),
            'sdk_enabled': True
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

if __name__ == '__main__':
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è...")
    init_lms_client()
    app.run(debug=True, host='0.0.0.0', port=5001)
