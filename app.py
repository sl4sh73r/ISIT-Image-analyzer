from flask import Flask, request, jsonify, render_template
import requests
import base64
import os
import time
from werkzeug.utils import secure_filename

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –∑–∞–≥—Ä—É–∑–æ–∫, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ LM Studio
LM_STUDIO_BASE_URL = "http://127.0.0.1:1234"
LM_STUDIO_URL = f"{LM_STUDIO_BASE_URL}/v1/chat/completions"
LM_STUDIO_MODELS_URL = f"{LM_STUDIO_BASE_URL}/v1/models"
LM_STUDIO_LOAD_MODEL_URL = f"{LM_STUDIO_BASE_URL}/v1/models/load"
MODELS = ["qwen/qwen3-vl-4b", "google/gemma-3-4b"]

ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'bmp', 'webp'}

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def get_loaded_model():
    """–ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â—É—é –ê–ö–¢–ò–í–ù–û –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ LM Studio"""
    try:
        response = requests.get(LM_STUDIO_MODELS_URL, timeout=5)
        response.raise_for_status()
        models_data = response.json()
        
        # LM Studio –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π,
        # –Ω–æ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–∞—è –≤ —Å–ø–∏—Å–∫–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ –ø–∞–º—è—Ç—å
        loaded_models = models_data.get('data', [])
        if loaded_models:
            # –ü–µ—Ä–≤–∞—è –º–æ–¥–µ–ª—å - —ç—Ç–æ –∞–∫—Ç–∏–≤–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è
            return loaded_models[0].get('id', None)
        return None
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
        return None

def check_if_model_actually_loaded(model_name):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ –ø–∞–º—è—Ç—å (–Ω–µ –ø—Ä–æ—Å—Ç–æ –≤ —Å–ø–∏—Å–∫–µ)"""
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è —Å–¥–µ–ª–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        payload = {
            "model": model_name,
            "messages": [{"role": "user", "content": "test"}],
            "max_tokens": 1,
            "temperature": 0.1
        }
        
        response = requests.post(LM_STUDIO_URL, json=payload, timeout=10)
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ - –≤–µ—Ä–Ω—ë—Ç 200
        # –ï—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑-–∑–∞ –ø–∞–º—è—Ç–∏ - –≤–µ—Ä–Ω—ë—Ç –æ—à–∏–±–∫—É
        if response.status_code == 200:
            return True
        else:
            return False
    except Exception as e:
        error_text = str(e)
        if "insufficient system resources" in error_text.lower():
            return False
        return False

def load_model(model_name):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –≤ LM Studio —á–µ—Ä–µ–∑ API"""
    try:
        print(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {model_name}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ —É–∂–µ —ç—Ç–∞ –º–æ–¥–µ–ª—å
        current_model = get_loaded_model()
        if current_model and model_name in current_model:
            print(f"‚úì –ú–æ–¥–µ–ª—å {model_name} —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return True
        
        # LM Studio –∏—Å–ø–æ–ª—å–∑—É–µ—Ç POST –∑–∞–ø—Ä–æ—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        # –§–æ—Ä–º–∞—Ç –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–µ—Ä—Å–∏–∏ LM Studio
        payload = {
            "model": model_name
        }
        
        # –ü—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤
        endpoints_to_try = [
            f"{LM_STUDIO_BASE_URL}/v1/models/load",
            f"{LM_STUDIO_BASE_URL}/api/v0/models/load",
            f"{LM_STUDIO_BASE_URL}/models/load",
        ]
        
        for endpoint in endpoints_to_try:
            try:
                print(f"–ü—Ä–æ–±—É—é —ç–Ω–¥–ø–æ–∏–Ω—Ç: {endpoint}")
                response = requests.post(endpoint, json=payload, timeout=30)
                
                if response.status_code == 200:
                    print(f"‚úì –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                    # –î–∞—ë–º –≤—Ä–µ–º—è –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å—Å—è
                    time.sleep(5)
                    return True
                elif response.status_code == 404:
                    # –≠—Ç–æ—Ç —ç–Ω–¥–ø–æ–∏–Ω—Ç –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π
                    continue
                else:
                    print(f"–û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞ ({response.status_code}): {response.text}")
            except requests.exceptions.RequestException as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ {endpoint}: {e}")
                continue
        
        # –ï—Å–ª–∏ API –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º False
        print(f"‚ö† API –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –≤—Ä—É—á–Ω—É—é.")
        return False
        
    except Exception as e:
        print(f"‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return False

def unload_model():
    """–í—ã–≥—Ä—É–∂–∞–µ—Ç —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å –∏–∑ LM Studio"""
    try:
        print("–ü–æ–ø—ã—Ç–∫–∞ –≤—ã–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å")
        
        endpoints_to_try = [
            f"{LM_STUDIO_BASE_URL}/v1/models/unload",
            f"{LM_STUDIO_BASE_URL}/api/v0/models/unload",
        ]
        
        for endpoint in endpoints_to_try:
            try:
                response = requests.post(endpoint, timeout=10)
                if response.status_code == 200:
                    print("‚úì –ú–æ–¥–µ–ª—å –≤—ã–≥—Ä—É–∂–µ–Ω–∞")
                    time.sleep(2)
                    return True
            except:
                continue
        
        print("‚ö† API –≤—ã–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
        return False
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤—ã–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return False

def get_entity_from_image(image_path, model_name, auto_load=False):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—É—â–Ω–æ—Å—Ç—å –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —á–µ—Ä–µ–∑ LM Studio —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    
    auto_load=False –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, —Ç.–∫. LM Studio –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç API –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç—å (–Ω–µ –ø—Ä–æ—Å—Ç–æ –≤ —Å–ø–∏—Å–∫–µ)
        current_model = get_loaded_model()
        
        # LM Studio –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—Å–µ –º–æ–¥–µ–ª–∏ –≤ —Å–ø–∏—Å–∫–µ, –Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–∞—è
        if not current_model or model_name not in current_model:
            print(f"‚ö† –ú–æ–¥–µ–ª—å {model_name} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∞–∫—Ç–∏–≤–Ω–æ–π")
            print(f"  –¢–µ–∫—É—â–∞—è –∞–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å: {current_model}")
            return {
                "error": f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ –ø–∞–º—è—Ç—å. –í—ã–≥—Ä—É–∑–∏—Ç–µ —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å '{current_model}' –∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ '{model_name}' –≤ LM Studio.",
                "requires_manual_load": True,
                "current_loaded": current_model
            }
        
        # –ß–∏—Ç–∞–µ–º –∏ –∫–æ–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ base64
        with open(image_path, "rb") as img_file:
            img_b64 = base64.b64encode(img_file.read()).decode("utf-8")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º MIME-—Ç–∏–ø
        ext = image_path.rsplit('.', 1)[1].lower()
        mime_type = f"image/{ext if ext != 'jpg' else 'jpeg'}"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ LM Studio
        payload = {
            "model": model_name,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:{mime_type};base64,{img_b64}"
                            }
                        },
                        {
                            "type": "text",
                            "text": "–û–ø—Ä–µ–¥–µ–ª–∏, —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–æ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ. –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º –∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–æ–π —Ñ—Ä–∞–∑–æ–π ‚Äî —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."
                        }
                    ]
                }
            ],
            "max_tokens": 30,
            "temperature": 0.2
        }
        
        # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –∑–∞–ø—Ä–æ—Å–∞
        start_time = time.time()
        
        response = requests.post(LM_STUDIO_URL, json=payload, timeout=60)
        response.raise_for_status()
        result = response.json()
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        end_time = time.time()
        processing_time = round(end_time - start_time, 3)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏
        entity = result["choices"][0]["message"]["content"].strip()
        
        # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = {
            "entity": entity,
            "model": model_name,
            "processing_time": processing_time,
            "temperature": 0.2,
            "max_tokens": 30
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–∞—Ö, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        if "usage" in result:
            usage = result["usage"]
            metrics["prompt_tokens"] = usage.get("prompt_tokens", 0)
            metrics["completion_tokens"] = usage.get("completion_tokens", 0)
            metrics["total_tokens"] = usage.get("total_tokens", 0)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (—Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É)
            if processing_time > 0 and metrics["completion_tokens"] > 0:
                metrics["tokens_per_second"] = round(metrics["completion_tokens"] / processing_time, 2)
        
        return metrics
        
    except requests.exceptions.RequestException as e:
        return {"error": f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ LM Studio: {str(e)}"}
    except Exception as e:
        return {"error": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}"}

@app.route('/')
def index():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
    return render_template('index.html')

@app.route('/api/check-models', methods=['GET'])
def check_models():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≤ LM Studio"""
    try:
        response = requests.get(LM_STUDIO_MODELS_URL, timeout=5)
        response.raise_for_status()
        models_data = response.json()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        loaded_models = [model.get('id', '') for model in models_data.get('data', [])]
        current_loaded = loaded_models[0] if loaded_models else None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π
        available_models = []
        for model_name in MODELS:
            is_loaded = any(model_name in model_id for model_id in loaded_models)
            available_models.append({
                'name': model_name,
                'short_name': model_name.split('/')[1] if '/' in model_name else model_name,
                'available': is_loaded,
                'currently_loaded': is_loaded and current_loaded and model_name in current_loaded
            })
        
        loaded_count = sum(1 for m in available_models if m['available'])
        
        return jsonify({
            'status': 'ok',
            'models': available_models,
            'loaded_count': loaded_count,
            'total_count': len(MODELS),
            'all_loaded': loaded_count == len(MODELS),
            'current_model': current_loaded,
            'auto_switching': True,  # –£–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –∞–≤—Ç–æ–ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ
            'note': '–ú–æ–¥–µ–ª–∏ –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ' if loaded_count < len(MODELS) else '–û–±–µ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã'
        })
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e),
            'suggestion': '–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ LM Studio –∑–∞–ø—É—â–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É http://127.0.0.1:1234'
        }), 500

@app.route('/api/active-model', methods=['GET'])
def get_active_model():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –∞–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å"""
    try:
        current = get_loaded_model()
        
        return jsonify({
            'success': True,
            'active_model': current,
            'active_model_short': current.split('/')[1] if current and '/' in current else current,
            'available_models': MODELS,
            'manual_switching_required': True,
            'instructions': {
                'step1': '–û—Ç–∫—Ä–æ–π—Ç–µ LM Studio',
                'step2': f'–í—ã–≥—Ä—É–∑–∏—Ç–µ —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å: {current}' if current else '–ó–∞–≥—Ä—É–∑–∏—Ç–µ –Ω—É–∂–Ω—É—é –º–æ–¥–µ–ª—å',
                'step3': '–ó–∞–≥—Ä—É–∑–∏—Ç–µ –Ω—É–∂–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ —Å–ø–∏—Å–∫–∞',
                'step4': '–í–µ—Ä–Ω–∏—Ç–µ—Å—å –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–Ω–æ–≤–∞'
            }
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/analyze-single', methods=['POST'])
def analyze_single_model():
    """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –æ–¥–Ω–æ–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    if 'image' not in request.files:
        return jsonify({'success': False, 'error': '–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}), 400
    
    if 'model' not in request.form:
        return jsonify({'success': False, 'error': '–ú–æ–¥–µ–ª—å –Ω–µ —É–∫–∞–∑–∞–Ω–∞'}), 400
    
    file = request.files['image']
    model_name = request.form['model']
    
    if file.filename == '':
        return jsonify({'success': False, 'error': '–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω'}), 400
    
    if not allowed_file(file.filename):
        return jsonify({'success': False, 'error': '–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞'}), 400
    
    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(filepath)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        current_model = get_loaded_model()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –Ω—É–∂–Ω–∞—è –º–æ–¥–µ–ª—å
        if not current_model or current_model != model_name:
            return jsonify({
                'success': False,
                'error': f'–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ LM Studio. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –µ—ë –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.',
                'current_model': current_model
            }), 400
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        result = get_entity_from_image(filepath, model_name)
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        if os.path.exists(filepath):
            os.remove(filepath)
        
        if 'error' in result:
            return jsonify({
                'success': False,
                'error': result['error']
            }), 500
        
        return jsonify({
            'success': True,
            'result': result
        })
        
    except Exception as e:
        if os.path.exists(filepath):
            os.remove(filepath)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/analyze', methods=['POST'])
def analyze_image():
    """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è - –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏"""
    if 'image' not in request.files:
        return jsonify({'error': '–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}), 400
    
    file = request.files['image']
    
    if file.filename == '':
        return jsonify({'error': '–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω'}), 400
    
    if not allowed_file(file.filename):
        return jsonify({'error': '–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞'}), 400
    
    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(filepath)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ–±–µ–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û
        # –ú–æ–¥–µ–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –∏ –≤—ã–≥—Ä—É–∂–∞—é—Ç—Å—è –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        results = []
        for model_name in MODELS:
            print(f"\n{'='*60}")
            print(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏: {model_name}")
            print(f"{'='*60}")
            
            # –í–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏
            result = get_entity_from_image(filepath, model_name, auto_load=True)
            
            if "error" not in result:
                results.append(result)
                print(f"‚úì {model_name}: {result.get('entity', 'N/A')}")
                print(f"  –í—Ä–µ–º—è: {result.get('processing_time', 'N/A')}—Å")
                print(f"  –¢–æ–∫–µ–Ω–æ–≤: {result.get('total_tokens', 'N/A')}")
            else:
                error_msg = result["error"]
                current_loaded = result.get("current_loaded", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏
                if result.get("requires_manual_load"):
                    model_short = model_name.split('/')[1] if '/' in model_name else model_name
                    current_short = current_loaded.split('/')[1] if current_loaded and '/' in current_loaded else current_loaded
                    
                    error_msg = f"–ú–æ–¥–µ–ª—å {model_short} –Ω–µ –∞–∫—Ç–∏–≤–Ω–∞. –°–µ–π—á–∞—Å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {current_short}"
                    instruction = f"–í LM Studio: –≤—ã–≥—Ä—É–∑–∏—Ç–µ '{current_short}' ‚Üí –∑–∞–≥—Ä—É–∑–∏—Ç–µ '{model_short}' ‚Üí –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –∞–Ω–∞–ª–∏–∑"
                    
                    results.append({
                        "model": model_name,
                        "error": error_msg,
                        "instruction": instruction,
                        "current_loaded": current_loaded,
                        "requires_manual_switch": True
                    })
                    print(f"‚úó {model_name}: {error_msg}")
                    print(f"  üí° {instruction}")
                else:
                    # –î—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏
                    if "400 Client Error" in error_msg or "Bad Request" in error_msg:
                        error_msg = f"–ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ {model_name} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –≤ LM Studio."
                    
                    results.append({
                        "model": model_name,
                        "error": error_msg
                    })
                    print(f"‚úó {model_name}: {error_msg}")
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏
            time.sleep(1)
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        os.remove(filepath)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —É—Å–ø–µ—à–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        successful_results = [r for r in results if "error" not in r]
        
        if not successful_results:
            return jsonify({
                'success': False,
                'error': '–û–±–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –∏–ª–∏ –≤–µ—Ä–Ω—É–ª–∏ –æ—à–∏–±–∫—É',
                'results': results,
                'suggestion': '–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ LM Studio'
            }), 500
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)
        comparison = calculate_comparison(results)
        
        return jsonify({
            'success': True,
            'results': results,
            'comparison': comparison,
            'models_analyzed': len(successful_results),
            'models_failed': len(results) - len(successful_results)
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

def calculate_comparison(results):
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏"""
    comparison = {}
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    successful_results = [r for r in results if "error" not in r]
    
    if len(successful_results) < 2:
        return comparison
    
    model1, model2 = successful_results[0], successful_results[1]
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    if "processing_time" in model1 and "processing_time" in model2:
        time_diff = abs(model1["processing_time"] - model2["processing_time"])
        faster_model = model1["model"] if model1["processing_time"] < model2["processing_time"] else model2["model"]
        comparison["time_difference"] = round(time_diff, 3)
        comparison["faster_model"] = faster_model
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞
        avg_time = (model1["processing_time"] + model2["processing_time"]) / 2
        if avg_time > 0:
            comparison["time_difference_percent"] = round((time_diff / avg_time) * 100, 1)
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
    if "tokens_per_second" in model1 and "tokens_per_second" in model2:
        faster_tokens_model = model1["model"] if model1["tokens_per_second"] > model2["tokens_per_second"] else model2["model"]
        comparison["faster_tokens_model"] = faster_tokens_model
        comparison["tokens_per_second_diff"] = round(abs(model1["tokens_per_second"] - model2["tokens_per_second"]), 2)
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
    if "total_tokens" in model1 and "total_tokens" in model2:
        comparison["total_tokens_diff"] = abs(model1["total_tokens"] - model2["total_tokens"])
        comparison["more_efficient_model"] = model1["model"] if model1["total_tokens"] < model2["total_tokens"] else model2["model"]
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤
    if "entity" in model1 and "entity" in model2:
        comparison["answers_match"] = model1["entity"].lower() == model2["entity"].lower()
        comparison["answer_similarity"] = "identical" if comparison["answers_match"] else "different"
    
    return comparison

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5001)
