from flask import Flask, request, jsonify, render_template
import requests
import base64
import os
import time
from werkzeug.utils import secure_filename
from dotenv import load_dotenv
from sklearn.metrics import confusion_matrix
import numpy as np
from flask_cors import CORS

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()

# –ü–æ–ª—É—á–µ–Ω–∏–µ API –∫–ª—é—á–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
API_KEY = os.getenv('API_KEY')

app = Flask(__name__)
CORS(app, origins=["*"], allow_headers=["*"], methods=["*"])  # –†–∞–∑—Ä–µ—à–∞–µ–º –≤—Å–µ origins, headers –∏ methods –¥–ª—è CORS
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –∑–∞–≥—Ä—É–∑–æ–∫, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ API
LM_STUDIO_BASE_URL = "https://llama.sndi.my"
LM_STUDIO_URL = f"{LM_STUDIO_BASE_URL}/api/v1/chat/completions"
LM_STUDIO_MODELS_URL = f"{LM_STUDIO_BASE_URL}/api/v1/models"
HEADERS = {"Authorization": f"Bearer {API_KEY}"}
MODELS = []  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω—è—Ç—å—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏–∑ API

ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'bmp', 'webp'}

def load_vision_models():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π vision –∏–∑ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ API"""
    global MODELS
    max_retries = 3
    retry_delay = 2
    
    # –ï—Å–ª–∏ –º–æ–¥–µ–ª–∏ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Ö
    if MODELS:
        return MODELS
    
    for attempt in range(max_retries):
        try:
            print(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π...")
            response = requests.get(LM_STUDIO_MODELS_URL, headers=HEADERS, timeout=15)
            response.raise_for_status()
            models_data = response.json()
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å vision
            all_models = models_data.get('data', [])
            vision_models = []
            
            for model in all_models:
                info = model.get('info', {})
                meta = info.get('meta', {})
                capabilities = meta.get('capabilities', {})
                
                if capabilities.get('vision', False):
                    vision_models.append(model['id'])
            
            MODELS = vision_models
            print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(MODELS)} –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π vision: {MODELS}")
            return MODELS  # –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∑–∏–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫
            
        except Exception as e:
            print(f"‚úó –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
            if attempt < max_retries - 1:
                print(f"‚è≥ –ñ–¥–µ–º {retry_delay} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                time.sleep(retry_delay)
            else:
                print("‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –º–æ–¥–µ–ª–∏")
    
    # Fallback –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏
    MODELS = ["Qwen3-VL-235B-A22B-Instruct", "google/gemma-3-27b-it"]
    print(f"‚ö† –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –º–æ–¥–µ–ª–∏: {MODELS}")
    return MODELS

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def get_loaded_model():
    """–î–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ API –º–æ–¥–µ–ª–∏ –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω—ã - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—É—é –∏–∑ —Å–ø–∏—Å–∫–∞"""
    return MODELS[0] if MODELS else None

def test_model_availability(model_name):
    """–î–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ API –º–æ–¥–µ–ª–∏ –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω—ã"""
    return model_name in MODELS

def check_if_model_actually_loaded(model_name):
    """–î–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ API –º–æ–¥–µ–ª–∏ –≤—Å–µ–≥–¥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã"""
    return model_name in MODELS

def load_model(model_name):
    """–î–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ API –º–æ–¥–µ–ª–∏ –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω—ã"""
    if model_name in MODELS:
        print(f"‚úì –ú–æ–¥–µ–ª—å {model_name} –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º API")
        return True
    else:
        print(f"‚úó –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Å–ø–∏—Å–∫–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö")
        return False

def unload_model():
    """–î–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ API –≤—ã–≥—Ä—É–∑–∫–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è"""
    print("–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π API: –≤—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
    return True

def get_entity_from_image(image_path, model_name, mode='description', classification_settings=None):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—É—â–Ω–æ—Å—Ç—å –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —á–µ—Ä–µ–∑ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π API"""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –æ–Ω–∏ –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã
        load_vision_models()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        if model_name not in MODELS:
            return {
                "error": f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º API"
            }

        # –ß–∏—Ç–∞–µ–º –∏ –∫–æ–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ base64
        with open(image_path, "rb") as img_file:
            img_b64 = base64.b64encode(img_file.read()).decode("utf-8")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º MIME-—Ç–∏–ø
        ext = image_path.rsplit('.', 1)[1].lower()
        mime_type = f"image/{ext if ext != 'jpg' else 'jpeg'}"

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
        if mode == 'classification' and classification_settings:
            positive_class = classification_settings.get('positiveClass', '–°–∞–º–æ–ª–µ—Ç')
            negative_class = classification_settings.get('negativeClass', '–ù–µ —Å–∞–º–æ–ª–µ—Ç')
            prompt_text = f"–û–ø—Ä–µ–¥–µ–ª–∏, —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–æ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ. –≠—Ç–æ {positive_class} –∏–ª–∏ {negative_class}? –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: '{positive_class}' –∏–ª–∏ '{negative_class}'."
        else:
            prompt_text = "–û–ø—Ä–µ–¥–µ–ª–∏, —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–æ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ. –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º –∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–æ–π —Ñ—Ä–∞–∑–æ–π ‚Äî —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º—É API
        payload = {
            "model": model_name,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:{mime_type};base64,{img_b64}"
                            }
                        },
                        {
                            "type": "text",
                            "text": prompt_text
                        }
                    ]
                }
            ],
            "max_tokens": 30,
            "temperature": 0.2
        }

        # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –∑–∞–ø—Ä–æ—Å–∞
        start_time = time.time()

        response = requests.post(LM_STUDIO_URL, json=payload, headers=HEADERS, timeout=120)
        response.raise_for_status()
        result = response.json()

        # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç API –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        print("[DEBUG] API Response:", result)

        # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        if "error" in result:
            print("[ERROR] API Error:", result["error"])

        # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        end_time = time.time()
        processing_time = round(end_time - start_time, 3)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏
        entity = result["choices"][0]["message"]["content"].strip()

        # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = {
            "entity": entity,
            "model": model_name,
            "processing_time": processing_time,
            "temperature": 0.2,
            "max_tokens": 30,
            "mode": mode,
            "model_info": {
                "name": model_name,
                "provider": model_name.split('/')[0] if '/' in model_name else 'corporate',
                "model_short": model_name.split('/')[1] if '/' in model_name else model_name,
                "api_endpoint": LM_STUDIO_URL,
                "request_type": "vision-language"
            }
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–∞—Ö, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        if "usage" in result:
            usage = result["usage"]
            metrics["prompt_tokens"] = usage.get("prompt_tokens", 0)
            metrics["completion_tokens"] = usage.get("completion_tokens", 0)
            metrics["total_tokens"] = usage.get("total_tokens", 0)

            # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (—Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É)
            if processing_time > 0 and metrics["completion_tokens"] > 0:
                metrics["tokens_per_second"] = round(metrics["completion_tokens"] / processing_time, 2)

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–ø—Ä–æ—Å–µ
        metrics["request_info"] = {
            "image_size": len(img_b64),
            "mime_type": mime_type,
            "api_response_time": processing_time,
            "status": "success"
        }

        return metrics

    except requests.exceptions.RequestException as e:
        return {"error": f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º—É API: {str(e)}"}
    except Exception as e:
        return {"error": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}"}

@app.route('/')
def index():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
    return render_template('index.html')

@app.route('/api/vlm-models', methods=['GET'])
def get_vlm_models():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö VLM (vision) –º–æ–¥–µ–ª–µ–π –∏–∑ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ API"""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –æ–Ω–∏ –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã
        load_vision_models()
        
        response = requests.get(LM_STUDIO_MODELS_URL, headers=HEADERS, timeout=10)
        response.raise_for_status()
        models_data = response.json()
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å vision
        all_models = models_data.get('data', [])
        vlm_models = []
        
        for model in all_models:
            info = model.get('info', {})
            meta = info.get('meta', {})
            capabilities = meta.get('capabilities', {})
            
            if capabilities.get('vision', False):
                vlm_models.append({
                    'id': model['id'],
                    'name': model['id'],
                    'publisher': model['id'].split('/')[0] if '/' in model['id'] else 'unknown',
                    'arch': 'unknown',
                    'state': 'loaded',
                    'quantization': '',
                    'max_context': model.get('max_model_len', 0),
                    'loaded': True
                })
        
        return jsonify({
            'status': 'ok',
            'models': vlm_models,
            'total': len(vlm_models),
            'loaded_count': len(vlm_models)
        })
    except requests.exceptions.Timeout:
        return jsonify({
            'status': 'error',
            'message': '–¢–∞–π–º–∞—É—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º—É API'
        }), 504
    except requests.exceptions.ConnectionError:
        return jsonify({
            'status': 'error',
            'message': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º—É API'
        }), 503
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': f'–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {str(e)}'
        }), 500

@app.route('/api/check-models', methods=['GET'])
def check_models():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≤ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º API"""
    try:
        # –í –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º API –≤—Å–µ –º–æ–¥–µ–ª–∏ –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω—ã
        available_models = []
        for model_name in MODELS:
            available_models.append({
                'name': model_name,
                'short_name': model_name.split('/')[1] if '/' in model_name else model_name,
                'available': True,
                'currently_loaded': True  # –í—Å–µ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
            })
        
        return jsonify({
            'status': 'ok',
            'models': available_models,
            'loaded_count': len(MODELS),
            'total_count': len(MODELS),
            'all_loaded': True,
            'current_model': MODELS[0],
            'auto_switching': True,
            'note': '–í—Å–µ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –≤ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º API'
        })
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e),
            'suggestion': '–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º—É API'
        }), 500

@app.route('/api/active-model', methods=['GET'])
def get_active_model():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –∞–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å"""
    try:
        current = get_loaded_model()
        
        return jsonify({
            'success': True,
            'active_model': current,
            'active_model_short': current.split('/')[1] if current and '/' in current else current,
            'available_models': MODELS,
            'manual_switching_required': False,  # –í –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º API –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ
            'instructions': {}
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/load-model', methods=['POST'])
def api_load_model():
    """–í –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º API –º–æ–¥–µ–ª–∏ –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω—ã"""
    data = request.get_json()
    model_id = data.get('model_id')
    
    if not model_id:
        return jsonify({'success': False, 'error': 'model_id –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω'}), 400
    
    if model_id in MODELS:
        return jsonify({
            'success': True,
            'message': f'–ú–æ–¥–µ–ª—å {model_id} –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º API',
            'already_loaded': True
        })
    else:
        return jsonify({
            'success': False,
            'error': f'–ú–æ–¥–µ–ª—å {model_id} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è'
        }), 400

@app.route('/api/unload-model', methods=['POST'])
def api_unload_model():
    """–í –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º API –≤—ã–≥—Ä—É–∑–∫–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è"""
    data = request.get_json()
    model_id = data.get('model_id')
    
    if not model_id:
        return jsonify({'success': False, 'error': 'model_id –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω'}), 400
    
    return jsonify({
        'success': True,
        'message': f'–ú–æ–¥–µ–ª—å {model_id} –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º API',
        'already_unloaded': True
    })

@app.route('/api/analyze', methods=['POST'])
def analyze_image():
    """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    if 'image' not in request.files:
        return jsonify({'error': '–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}), 400
    
    file = request.files['image']
    model_name = request.form.get('model')
    mode = request.form.get('mode', 'description')
    positive_class = request.form.get('positiveClass', '–°–∞–º–æ–ª–µ—Ç')
    negative_class = request.form.get('negativeClass', '–ù–µ —Å–∞–º–æ–ª–µ—Ç')
    ground_truth = request.form.get('groundTruth', '')  # –î–ª—è —Ä–µ–∂–∏–º–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    
    if file.filename == '':
        return jsonify({'error': '–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω'}), 400
    
    if not model_name:
        return jsonify({'error': '–ú–æ–¥–µ–ª—å –Ω–µ —É–∫–∞–∑–∞–Ω–∞'}), 400
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å
    if model_name not in MODELS:
        return jsonify({'error': f'–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è'}), 400
    
    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(filepath)
        
        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            classification_settings = None
            if mode == 'classification':
                classification_settings = {
                    'positiveClass': positive_class,
                    'negativeClass': negative_class
                }
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
            result = get_entity_from_image(filepath, model_name, mode, classification_settings)
            
            if "error" in result:
                response_data = {
                    'success': False,
                    'results': [{
                        'index': 0,
                        'filename': filename,
                        'success': False,
                        'error': result["error"],
                        'current_loaded': result.get("current_loaded"),
                        'requires_manual_switch': result.get("requires_manual_load", False)
                    }]
                }
            else:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ –≤ —Ä–µ–∂–∏–º–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                is_correct = None
                if mode == 'classification' and ground_truth:
                    entity_lower = result.get('entity', '').lower().strip()
                    positive_lower = positive_class.lower().strip()
                    negative_lower = negative_class.lower().strip()
                    
                    print(f"[DEBUG] Classification check:")
                    print(f"  Entity: '{entity_lower}'")
                    print(f"  Ground truth: '{ground_truth}'")
                    print(f"  Positive class: '{positive_lower}'")
                    print(f"  Negative class: '{negative_lower}'")
                    
                    # –õ–æ–≥–∏–∫–∞: –ø—Ä–æ–≤–µ—Ä—è–µ–º, –∫ –∫–∞–∫–æ–º—É –∫–ª–∞—Å—Å—É –±–ª–∏–∂–µ –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
                    if ground_truth == 'positive':
                        # –û–∂–∏–¥–∞–µ–º –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç –±–æ–ª—å—à–µ –ø–æ—Ö–æ–∂ –Ω–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å
                        positive_match = positive_lower in entity_lower and not (negative_lower in entity_lower and entity_lower.startswith(negative_lower.split()[0]))
                        is_correct = positive_match
                    elif ground_truth == 'negative':
                        # –û–∂–∏–¥–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç –±–æ–ª—å—à–µ –ø–æ—Ö–æ–∂ –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å
                        negative_match = negative_lower in entity_lower or entity_lower.startswith(negative_lower.split()[0])
                        is_correct = negative_match
                    else:
                        is_correct = False
                    
                    print(f"  Result: is_correct = {is_correct}")
                
                
                response_data = {
                    'success': True,
                    'results': [{
                        'index': 0,
                        'filename': filename,
                        'success': True,
                        'entity': result.get('entity', 'N/A'),
                        'processing_time': result.get('processing_time', 0),
                        'tokens_per_second': result.get('tokens_per_second'),
                        'total_tokens': result.get('total_tokens'),
                        'model': model_name,
                        'mode': mode,
                        'classification_correct': is_correct,
                        'ground_truth': ground_truth if mode == 'classification' else None
                    }]
                }
                
        except Exception as e:
            response_data = {
                'success': False,
                'results': [{
                    'index': 0,
                    'filename': filename,
                    'success': False,
                    'error': str(e)
                }]
            }
        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            if os.path.exists(filepath):
                os.remove(filepath)
        
        return jsonify(response_data)
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/get-mode-settings', methods=['GET'])
def get_mode_settings():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã"""
    return jsonify({
        'currentMode': 'description',  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        'classificationSettings': {
            'positiveClass': '–°–∞–º–æ–ª–µ—Ç',
            'negativeClass': '–ù–µ —Å–∞–º–æ–ª–µ—Ç'
        }
    })

@app.route('/api/model-comparison', methods=['POST'])
def get_model_comparison():
    """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
    data = request.get_json()
    results = data.get('results', [])
    mode = data.get('mode', 'description')
    classification_settings = data.get('classificationSettings', {})
    ground_truth_data = data.get('groundTruth', {})  # –°–ª–æ–≤–∞—Ä—å filename -> ground_truth

    if not results:
        return jsonify({'error': '–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞'}), 400

    try:
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
        image_results = {}
        model_names = set()

        for result in results:
            image_name = result.get('filename', 'unknown')
            model_name = result.get('model', 'unknown')
            entity = result.get('entity', '')
            success = result.get('success', False)

            model_names.add(model_name)

            if image_name not in image_results:
                image_results[image_name] = {}

            image_results[image_name][model_name] = {
                'entity': entity,
                'success': success,
                'processing_time': result.get('processing_time', 0),
                'tokens_per_second': result.get('tokens_per_second', 0),
                'total_tokens': result.get('total_tokens', 0)
            }

        model_names = sorted(list(model_names))

        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        comparison_metrics = {
            'total_images': len(image_results),
            'models_compared': len(model_names),
            'model_names': model_names,
            'agreement_matrix': [],
            'performance_metrics': {},
            'mode': mode
        }

        # –î–ª—è —Ä–µ–∂–∏–º–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤
        if mode == 'classification':
            positive_class = classification_settings.get('positiveClass', '–°–∞–º–æ–ª–µ—Ç')
            negative_class = classification_settings.get('negativeClass', '–ù–µ —Å–∞–º–æ–ª–µ—Ç')
            comparison_metrics['classification_settings'] = {
                'positive_class': positive_class,
                'negative_class': negative_class
            }

        # –ú–∞—Ç—Ä–∏—Ü–∞ —Å–æ–≥–ª–∞—Å–∏—è (confusion matrix –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤)
        agreement_matrix = []
        for i, model1 in enumerate(model_names):
            row = []
            for j, model2 in enumerate(model_names):
                if i == j:
                    # –î–∏–∞–≥–æ–Ω–∞–ª—å - —É—Å–ø–µ—à–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏
                    successful_answers = sum(1 for img_results in image_results.values()
                                           if img_results.get(model1, {}).get('success', False))
                    row.append(successful_answers)
                else:
                    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –¥–≤—É—Ö –º–æ–¥–µ–ª–µ–π
                    agreements = 0
                    for img_results in image_results.values():
                        model1_result = img_results.get(model1, {})
                        model2_result = img_results.get(model2, {})

                        if (model1_result.get('success', False) and
                            model2_result.get('success', False) and
                            model1_result.get('entity', '').lower() == model2_result.get('entity', '').lower()):
                            agreements += 1
                    row.append(agreements)
            agreement_matrix.append(row)

        comparison_metrics['agreement_matrix'] = agreement_matrix

        # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        for model_name in model_names:
            model_times = []
            model_tokens_per_sec = []
            model_total_tokens = []
            successful_count = 0
            correct_predictions = 0  # –î–ª—è —Ä–µ–∂–∏–º–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

            for img_name, img_results in image_results.items():
                model_result = img_results.get(model_name, {})
                if model_result.get('success', False):
                    successful_count += 1
                    model_times.append(model_result.get('processing_time', 0))
                    if model_result.get('tokens_per_second', 0) > 0:
                        model_tokens_per_sec.append(model_result.get('tokens_per_second', 0))
                    model_total_tokens.append(model_result.get('total_tokens', 0))

                    # –î–ª—è —Ä–µ–∂–∏–º–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å
                    if mode == 'classification':
                        ground_truth = ground_truth_data.get(img_name)
                        if ground_truth:
                            entity_lower = model_result.get('entity', '').lower().strip()
                            positive_lower = positive_class.lower()
                            negative_lower = negative_class.lower()
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ –æ—Ç–≤–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –∫–ª–∞—Å—Å—É
                            if ground_truth == 'positive':
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç –±–æ–ª—å—à–µ –ø–æ—Ö–æ–∂ –Ω–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å
                                positive_match = positive_lower in entity_lower and not (negative_lower in entity_lower and entity_lower.startswith(negative_lower.split()[0]))
                                if positive_match:
                                    correct_predictions += 1
                            elif ground_truth == 'negative':
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç –±–æ–ª—å—à–µ –ø–æ—Ö–æ–∂ –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å  
                                negative_match = negative_lower in entity_lower or entity_lower.startswith(negative_lower.split()[0])
                                if negative_match:
                                    correct_predictions += 1

            comparison_metrics['performance_metrics'][model_name] = {
                'successful_predictions': successful_count,
                'total_predictions': len(image_results),
                'success_rate': round(successful_count / len(image_results) * 100, 2) if image_results else 0,
                'avg_processing_time': round(sum(model_times) / len(model_times), 3) if model_times else 0,
                'avg_tokens_per_second': round(sum(model_tokens_per_sec) / len(model_tokens_per_sec), 2) if model_tokens_per_sec else 0,
                'total_tokens_used': sum(model_total_tokens),
                'avg_tokens_used': round(sum(model_total_tokens) / len(model_total_tokens), 1) if model_total_tokens else 0
            }

            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–µ–∂–∏–º–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            if mode == 'classification':
                comparison_metrics['performance_metrics'][model_name]['correct_predictions'] = correct_predictions
                comparison_metrics['performance_metrics'][model_name]['accuracy'] = round(correct_predictions / len(image_results) * 100, 2) if image_results else 0

        return jsonify(comparison_metrics)

    except Exception as e:
        return jsonify({'error': str(e)}), 500

def calculate_comparison(results):
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏"""
    comparison = {}
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    successful_results = [r for r in results if "error" not in r]
    
    if len(successful_results) < 2:
        return comparison
    
    model1, model2 = successful_results[0], successful_results[1]
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    if "processing_time" in model1 and "processing_time" in model2:
        time_diff = abs(model1["processing_time"] - model2["processing_time"])
        faster_model = model1["model"] if model1["processing_time"] < model2["processing_time"] else model2["model"]
        comparison["time_difference"] = round(time_diff, 3)
        comparison["faster_model"] = faster_model
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞
        avg_time = (model1["processing_time"] + model2["processing_time"]) / 2
        if avg_time > 0:
            comparison["time_difference_percent"] = round((time_diff / avg_time) * 100, 1)
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
    if "tokens_per_second" in model1 and "tokens_per_second" in model2:
        faster_tokens_model = model1["model"] if model1["tokens_per_second"] > model2["tokens_per_second"] else model2["model"]
        comparison["faster_tokens_model"] = faster_tokens_model
        comparison["tokens_per_second_diff"] = round(abs(model1["tokens_per_second"] - model2["tokens_per_second"]), 2)
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
    if "total_tokens" in model1 and "total_tokens" in model2:
        comparison["total_tokens_diff"] = abs(model1["total_tokens"] - model2["total_tokens"])
        comparison["more_efficient_model"] = model1["model"] if model1["total_tokens"] < model2["total_tokens"] else model2["model"]
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤
    if "entity" in model1 and "entity" in model2:
        comparison["answers_match"] = model1["entity"].lower() == model2["entity"].lower()
        comparison["answer_similarity"] = "identical" if comparison["answers_match"] else "different"
    
    return comparison

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5003)
